{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import boxcox\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "## Load data\n",
    "properties = pd.read_csv('data/properties_2016.csv')\n",
    "train = pd.read_csv('data/train_2016_v2.csv')\n",
    "all_data = properties.merge(train, how='right', on='parcelid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Data Types\n",
    "\n",
    "We'll convert all categorical values so that the missing value is a feature in itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deal with Na values and adjust type\n",
    "def categorize(s, na_value = 0):\n",
    "    return s.astype('category').cat.add_categories(na_value).fillna(na_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adjust categorical variables\n",
    "def preprocess(df, fill_value):\n",
    "    \n",
    "    # Null Proportion Feature\n",
    "    null_prop = df.apply(lambda x: x.count(), axis=1)/df.shape[1]\n",
    "    \n",
    "    df['airconditioningtypeid'] = categorize(df['airconditioningtypeid'])\n",
    "    df['architecturalstyletypeid'] = categorize(df['architecturalstyletypeid'])\n",
    "    df['buildingclasstypeid'] = categorize(df['buildingclasstypeid'])\n",
    "    df['buildingqualitytypeid'] = categorize(df['buildingqualitytypeid'])\n",
    "    df['hasdeck'] = df['decktypeid'].notnull()\n",
    "    df.drop('decktypeid', axis=1, inplace=True)\n",
    "\n",
    "    df['fireplaceflag'] = df['fireplaceflag'] == True\n",
    "    df['hashottuborspa'] = df['hashottuborspa'] == True\n",
    "    df['heatingorsystemtypeid'] = categorize(df['heatingorsystemtypeid'])\n",
    "    df.drop(['pooltypeid10','pooltypeid2','pooltypeid7'], axis=1, inplace=True)\n",
    "\n",
    "    df['propertycountylandusecode'] = categorize(df['propertycountylandusecode'],na_value='0000')\n",
    "    df['propertylandusetypeid'] = categorize(df['propertylandusetypeid'])\n",
    "    df['propertyzoningdesc'] = categorize(df['propertyzoningdesc'],'0000')\n",
    "    df.drop(['rawcensustractandblock','censustractandblock'],axis=1,inplace=True)\n",
    "\n",
    "    df['regionidcounty'] = categorize(df['regionidcounty'])\n",
    "    df['regionidcity'] = categorize(df['regionidcity'])\n",
    "    df['regionidzip'] = categorize(df['regionidzip'])\n",
    "    df['regionidneighborhood'] = categorize(df['regionidneighborhood'])\n",
    "    df['storytypeid'] = categorize(df['storytypeid'])\n",
    "    df['typeconstructiontypeid'] = categorize(df['typeconstructiontypeid'])\n",
    "    \n",
    "    object_data = df.dtypes == 'category'\n",
    "    \n",
    "    numeric_columns = ['bathroomcnt', 'bedroomcnt', 'calculatedbathnbr', \n",
    "                       'calculatedfinishedsquarefeet', 'finishedsquarefeet12',\n",
    "                       'fullbathcnt', 'latitude', 'longitude', 'lotsizesquarefeet', \n",
    "                       'roomcnt','yearbuilt', 'structuretaxvaluedollarcnt', 'taxvaluedollarcnt',\n",
    "                       'assessmentyear', 'landtaxvaluedollarcnt', 'taxamount']\n",
    "    train_numeric = df.loc[:, numeric_columns]\n",
    "    train_numeric.fillna(fill_value)\n",
    "    train_numeric['calculatedfinishedsquarefeet'] = np.log(train_numeric['calculatedfinishedsquarefeet'])\n",
    "    train_numeric['finishedsquarefeet12'] = np.log(train_numeric['finishedsquarefeet12'])[0]\n",
    "    train_numeric['lotsizesquarefeet'] = np.log(train_numeric['lotsizesquarefeet'])\n",
    "    train_numeric['structuretaxvaluedollarcnt'] = np.log(train_numeric['structuretaxvaluedollarcnt'])[0]\n",
    "    train_numeric['taxvaluedollarcnt'] = np.log(train_numeric['taxvaluedollarcnt'])[0]\n",
    "    train_numeric['landtaxvaluedollarcnt'] = np.log(train_numeric['landtaxvaluedollarcnt'])[0]\n",
    "    train_numeric['taxamount'] = np.log(train_numeric['taxamount'])[0]\n",
    "    train_numeric['null_prop'] = null_prop\n",
    "    \n",
    "    category_train = df.loc[:, object_data]\n",
    "    category_train = pd.get_dummies(category_train.drop(['regionidneighborhood','regionidcity','regionidcounty','propertycountylandusecode','propertyzoningdesc','buildingqualitytypeid','regionidzip'],axis=1))\n",
    "    category_train['buildingqualitytypeid'] = df['buildingqualitytypeid']\n",
    "    \n",
    "    X_train = pd.DataFrame(train_numeric).join(category_train)\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outlier Removal\n",
    "q1 = all_data['logerror'].quantile(0.25)\n",
    "q3 = all_data['logerror'].quantile(0.75)\n",
    "low_outlier = q1 - 3*(q3-q1)\n",
    "high_outlier = q3 + 3*(q3-q1)\n",
    "outlier_index = all_data[(all_data['logerror'] < low_outlier) | (all_data['logerror'] > high_outlier)].index\n",
    "all_data = all_data.drop(outlier_index, axis=0)\n",
    "\n",
    "fill_value = all_data.drop(['logerror','transactiondate'], axis=1).mean()\n",
    "X_train = preprocess(all_data.drop(['logerror','transactiondate'], axis=1), fill_value)\n",
    "X_train['month'] = pd.to_datetime(all_data['transactiondate']).dt.month\n",
    "y_train = all_data['logerror']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = preprocess(properties.drop('parcelid',axis=1), fill_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Modified Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test.to_csv('data/modified_testV1.csv', index=False)\n",
    "X_train.to_csv('data/modified_trainV1.csv', index=False)\n",
    "y_train.to_csv('data/labels.csv', index=False)\n",
    "properties['parcelid'].to_csv('data/parcelid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Some Regressor Models\n",
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.044055953384106089"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 3, 'min_samples_split': 3, 'max_depth': 13, 'min_impurity_split': 0.001}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized Search has provided us with these optimal parameters:\n",
    "* Max Depth: 13\n",
    "* Min impurity Spit: .001,\n",
    "* Min Samples Leaf: 3\n",
    "* Min Samples Split: 3\n",
    "\n",
    "With 150 Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=13,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=0.001, min_samples_leaf=3,\n",
       "           min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=150, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rf_optimal = rf_search.best_estimator_\n",
    "rf_optimal = RandomForestRegressor(verbose=True, n_estimators=150, max_depth=13, min_impurity_split=0.001, min_samples_leaf=3, min_samples_split=3)\n",
    "rf_optimal.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.0s finished\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(classifier, test_set, train_set, n_splits=25):\n",
    "    splitted = np.array_split(test_set.drop(set(test_set.columns) - set(train_set.columns),axis=1), n_splits)\n",
    "    oct_preds = []\n",
    "    nov_preds = []\n",
    "    dec_preds = []\n",
    "\n",
    "    cols = splitted[0].shape[1]\n",
    "\n",
    "    count = 0\n",
    "    for split in splitted:\n",
    "        count += 1\n",
    "        print(\"Predicting On {}\".format(count))\n",
    "        split = np.append(split, np.full([len(split),1], 10), axis=1)\n",
    "        oct_pred = rf_optimal.predict(split)\n",
    "\n",
    "        split[:, cols] = np.full(len(split), 11)\n",
    "        nov_pred = rf_optimal.predict(split)\n",
    "\n",
    "        split[:, cols] = np.full(len(split), 12)\n",
    "        dec_pred = rf_optimal.predict(split)\n",
    "\n",
    "        oct_preds.append(oct_pred)\n",
    "        nov_preds.append(nov_pred)\n",
    "        dec_preds.append(dec_pred)\n",
    "\n",
    "    rf10_predictions = np.hstack(oct_preds)\n",
    "    rf11_predictions = np.hstack(nov_preds)\n",
    "    rf12_predictions = np.hstack(dec_preds)\n",
    "    \n",
    "    return rf10_predictions, rf11_predictions, rf12_predictions\n",
    "\n",
    "rf10_predictions, rf11_predictions, rf12_predictions = get_predictions(rf_optimal, X_test, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing results to disk ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n",
    "        '201610': rf10_predictions, '201611': rf11_predictions, '201612': rf12_predictions,\n",
    "        '201710': rf10_predictions, '201711': rf11_predictions, '201712': rf12_predictions})\n",
    "# set col 'ParceID' to first col\n",
    "cols = output.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "output = output[cols]\n",
    "from datetime import datetime\n",
    "\n",
    "print( \"\\nWriting results to disk ...\" )\n",
    "output.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### AdaBoost\n",
    "ada = AdaBoostRegressor(n_estimators = 250)\n",
    "\n",
    "ada_param_dists = {'learning_rate':[0.01,.02,.04,.08,.12,.16,.2,\n",
    "                                   0.25,0.35,0.5,0.75,1.0,1.25,1.5,2]}\n",
    "\n",
    "ada_optimal = RandomizedSearchCV(ada, ada_param_dists, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.02}\n",
      "-0.0444358096508\n"
     ]
    }
   ],
   "source": [
    "ada_optimal.fit(X_train, y_train)\n",
    "print(ada_optimal.best_params_)\n",
    "print(ada_optimal.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'max_depth': [2, 3, 4, 5], 'min_samples_split': [2, 3, 4, 5], 'min_samples_leaf': [1, 2, 3], 'min_impurity_split': [0.001, 0.0001, 1e-05, 1e-06, 1e-07, 1e-08, 1e-09, 1e-10], 'loss': ['ls', 'lad', 'huber'], 'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='neg_mean_absolute_error',\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gbm_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4183062a8e46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbm_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbm_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgbm_optimal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbm_optimal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gbm_search' is not defined"
     ]
    }
   ],
   "source": [
    "print(gbm_search.best_params_)\n",
    "print(gbm_search.best_score_)\n",
    "gbm_optimal = gbm_optimal.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='lad', max_depth=5, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-05,\n",
       "             min_samples_leaf=2, min_samples_split=5,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_optimal.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting On 1\n",
      "Predicting On 2\n",
      "Predicting On 3\n",
      "Predicting On 4\n",
      "Predicting On 5\n",
      "Predicting On 6\n",
      "Predicting On 7\n",
      "Predicting On 8\n",
      "Predicting On 9\n",
      "Predicting On 10\n",
      "Predicting On 11\n",
      "Predicting On 12\n",
      "Predicting On 13\n",
      "Predicting On 14\n",
      "Predicting On 15\n",
      "Predicting On 16\n",
      "Predicting On 17\n",
      "Predicting On 18\n",
      "Predicting On 19\n",
      "Predicting On 20\n",
      "Predicting On 21\n",
      "Predicting On 22\n",
      "Predicting On 23\n",
      "Predicting On 24\n",
      "Predicting On 25\n"
     ]
    }
   ],
   "source": [
    "gb10_predictions, gb11_predictions, gb12_predictions = get_predictions(gbm_optimal, X_test, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing results to disk ...\n"
     ]
    }
   ],
   "source": [
    "output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n",
    "        '201610': gbm_predictions, '201611': gbm_predictions, '201612': gbm_predictions,\n",
    "        '201710': gbm_predictions, '201711': gbm_predictions, '201712': gbm_predictions})\n",
    "# set col 'ParceID' to first col\n",
    "cols = output.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "output = output[cols]\n",
    "from datetime import datetime\n",
    "\n",
    "print( \"\\nWriting results to disk ...\" )\n",
    "output.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2985217, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = LinearRegression()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
